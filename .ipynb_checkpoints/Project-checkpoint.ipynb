{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Boop boop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import importlib\n",
    "\n",
    "import utils\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles = utils.load_articles('wikitext-103-raw/wiki.train.raw')\n",
    "valid_articles = utils.load_articles('wikitext-103-raw/wiki.valid.raw')\n",
    "test_articles = utils.load_articles('wikitext-103-raw/wiki.test.raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "char_encoding = utils.get_encoding(train_articles + valid_articles + test_articles)\n",
    "char2int, int2char, int2hot, str2hot, hot2int, hot2str = char_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string with words'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot2str(str2hot(\"This is a string with words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(models)\n",
    "\n",
    "rnn_net_50 = models.RNNModule(len(char2int), 50)\n",
    "rnn_net_100 = models.RNNModule(len(char2int), 100)\n",
    "\n",
    "lstm_net_50 = models.LSTMModule(len(char2int), 50)\n",
    "lstm_net_100 = models.LSTMModule(len(char2int), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.3659, -8.3882, -8.5859,  ..., -8.7770, -8.5297, -8.7343],\n",
      "        [-8.4545, -8.6247, -8.5470,  ..., -8.6887, -8.5007, -8.4336],\n",
      "        [-8.4106, -8.4212, -8.5805,  ..., -8.7258, -8.5286, -8.5384],\n",
      "        ...,\n",
      "        [-8.3771, -8.5564, -8.6232,  ..., -8.5743, -8.7012, -8.5818],\n",
      "        [-8.3444, -8.5152, -8.6012,  ..., -8.7393, -8.5903, -8.3186],\n",
      "        [-8.3837, -8.5544, -8.6203,  ..., -8.5691, -8.6939, -8.5801]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([943, 467, 943,  ..., 943, 467, 943])\n",
      "tensor([[-8.2836, -8.2288, -8.6600,  ..., -8.5270, -8.2846, -8.6203],\n",
      "        [-8.3657, -8.7497, -8.6082,  ..., -8.5486, -8.4919, -8.4992],\n",
      "        [-8.4653, -8.4236, -8.4622,  ..., -8.7371, -8.6658, -8.4893],\n",
      "        ...,\n",
      "        [-8.3092, -8.5465, -8.5180,  ..., -8.5328, -8.7367, -8.5855],\n",
      "        [-8.2882, -8.5193, -8.5259,  ..., -8.6900, -8.6037, -8.3429],\n",
      "        [-8.3225, -8.5474, -8.5200,  ..., -8.5230, -8.7276, -8.5929]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([ 943, 2096,  943,  ...,  943,  467,  943])\n",
      "tensor([[-8.4786, -8.3029, -8.6078,  ..., -8.8552, -8.3405, -8.7069],\n",
      "        [-8.3601, -8.6419, -8.5800,  ..., -8.5407, -8.5807, -8.4730],\n",
      "        [-8.3586, -8.4168, -8.5410,  ..., -8.6420, -8.6221, -8.4282],\n",
      "        ...,\n",
      "        [-8.2666, -8.5566, -8.3446,  ..., -8.5275, -8.9740, -8.6888],\n",
      "        [-8.2578, -8.5388, -8.3500,  ..., -8.6527, -8.8795, -8.5189],\n",
      "        [-8.2706, -8.5562, -8.3426,  ..., -8.5233, -8.9716, -8.6900]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([ 943, 2096,  943,  ...,  943,  467,  943])\n",
      "tensor([[-8.2401, -8.2926, -8.5418,  ..., -8.7372, -8.3624, -8.5206],\n",
      "        [-8.4360, -8.7324, -8.5454,  ..., -8.5828, -8.5916, -8.5773],\n",
      "        [-8.3697, -8.3668, -8.3892,  ..., -8.7850, -8.7297, -8.4835],\n",
      "        ...,\n",
      "        [-8.3878, -8.4519, -8.1141,  ..., -8.5833, -9.1866, -8.9056],\n",
      "        [-8.3911, -8.4460, -8.1168,  ..., -8.6421, -9.1615, -8.8409],\n",
      "        [-8.3865, -8.4527, -8.1158,  ..., -8.5754, -9.1873, -8.9173]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([ 943, 2096,  943,  ...,  943,  467,  943])\n",
      "tensor([[-8.4069, -8.3187, -8.6311,  ..., -8.7003, -8.1442, -8.8699],\n",
      "        [-8.3460, -8.7081, -8.5035,  ..., -8.5450, -8.6247, -8.4536],\n",
      "        [-8.4350, -8.5333, -8.4888,  ..., -8.6658, -8.7595, -8.5718],\n",
      "        ...,\n",
      "        [-8.5920, -8.5276, -7.9618,  ..., -8.7203, -9.2166, -8.9369],\n",
      "        [-8.5939, -8.5326, -7.9710,  ..., -8.7654, -9.2146, -8.8880],\n",
      "        [-8.5920, -8.5276, -7.9617,  ..., -8.7203, -9.2166, -8.9370]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([ 943, 2096,  943,  ...,  943,  467,  943])\n",
      "tensor([[-8.3339, -8.3190, -8.6608,  ..., -8.7029, -8.2442, -8.7077],\n",
      "        [-8.4449, -8.8255, -8.5370,  ..., -8.4981, -8.5400, -8.4897],\n",
      "        [-8.4660, -8.3895, -8.3397,  ..., -8.7525, -8.8119, -8.5111],\n",
      "        ...,\n",
      "        [-8.7745, -8.5492, -7.8985,  ..., -8.8033, -9.3196, -9.0138],\n",
      "        [-8.7799, -8.5552, -7.9080,  ..., -8.8269, -9.3218, -8.9753],\n",
      "        [-8.7746, -8.5493, -7.8984,  ..., -8.8033, -9.3197, -9.0138]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([ 943, 2096,  943,  ...,  943,  467,  943])\n",
      "tensor([[-8.4378, -8.3953, -8.4969,  ..., -8.7811, -8.4389, -8.6123],\n",
      "        [-8.4092, -8.6863, -8.4625,  ..., -8.5268, -8.6715, -8.4906],\n",
      "        [-8.4854, -8.4761, -8.3252,  ..., -8.6792, -8.9318, -8.6373],\n",
      "        ...,\n",
      "        [-8.8904, -8.6685, -8.0011,  ..., -8.8914, -9.3612, -9.1391],\n",
      "        [-8.8939, -8.6726, -8.0113,  ..., -8.9004, -9.3588, -9.1079],\n",
      "        [-8.8904, -8.6685, -8.0011,  ..., -8.8914, -9.3612, -9.1391]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([ 943, 2096,  943,  ...,  943,  467,  943])\n",
      "tensor([[-8.3064, -8.3523, -8.4568,  ..., -9.0121, -8.4537, -8.5056],\n",
      "        [-8.4107, -8.7588, -8.5344,  ..., -8.4704, -8.6219, -8.4508],\n",
      "        [-8.5016, -8.4016, -8.3687,  ..., -8.7067, -8.8554, -8.6408],\n",
      "        ...,\n",
      "        [-8.9974, -8.7774, -8.1276,  ..., -9.0459, -9.4749, -9.2780],\n",
      "        [-9.0012, -8.7748, -8.1375,  ..., -9.0413, -9.4681, -9.2549],\n",
      "        [-8.9974, -8.7775, -8.1276,  ..., -9.0459, -9.4749, -9.2780]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([ 943, 2096,  943,  ...,  943,  467,  943])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-f6e87300c2ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train rnn_net_50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_net_50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/deep-project/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, articles, char_encoding)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train rnn_net_50\n",
    "models.train(rnn_net_50, train_articles, char_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train rnn_net_100\n",
    "models.train(rnn_net_100, train_articles, char_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.6006, -8.6423, -8.5574,  ..., -8.8246, -8.4354, -8.8200],\n",
      "        [-8.5514, -8.5638, -8.6003,  ..., -8.7144, -8.4416, -8.7473],\n",
      "        [-8.5501, -8.5544, -8.5815,  ..., -8.6641, -8.4465, -8.7119],\n",
      "        ...,\n",
      "        [-8.5768, -8.5300, -8.4962,  ..., -8.5786, -8.4506, -8.6283],\n",
      "        [-8.5741, -8.5126, -8.5064,  ..., -8.5873, -8.4521, -8.6359],\n",
      "        [-8.5755, -8.5303, -8.4935,  ..., -8.5745, -8.4507, -8.6275]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([943, 467, 943,  ..., 943, 467, 943])\n",
      "tensor([[-8.3388, -8.6511, -8.6120,  ..., -8.3629, -8.5457, -8.5766],\n",
      "        [-8.4492, -8.5891, -8.5365,  ..., -8.4939, -8.5252, -8.6435],\n",
      "        [-8.5274, -8.5496, -8.5280,  ..., -8.5609, -8.5182, -8.6592],\n",
      "        ...,\n",
      "        [-8.5924, -8.5228, -8.4975,  ..., -8.6181, -8.4194, -8.6725],\n",
      "        [-8.5891, -8.5068, -8.5072,  ..., -8.6263, -8.4209, -8.6803],\n",
      "        [-8.5913, -8.5225, -8.4948,  ..., -8.6146, -8.4199, -8.6714]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([ 943, 2096,  943,  ...,  943,  467,  943])\n",
      "tensor([[-8.3832, -8.5641, -8.3108,  ..., -8.7780, -8.3372, -8.7419],\n",
      "        [-8.4711, -8.5151, -8.3902,  ..., -8.7528, -8.3730, -8.6998],\n",
      "        [-8.5196, -8.5053, -8.4404,  ..., -8.7347, -8.3940, -8.6828],\n",
      "        ...,\n",
      "        [-8.5987, -8.4918, -8.4916,  ..., -8.7033, -8.3302, -8.7526],\n",
      "        [-8.5981, -8.4764, -8.5002,  ..., -8.7111, -8.3337, -8.7593],\n",
      "        [-8.5974, -8.4909, -8.4874,  ..., -8.7001, -8.3314, -8.7510]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([ 943, 2096,  943,  ...,  943,  467,  943])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-ace8568d63f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train lstm_net_50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_net_50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/deep-project/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, articles, char_encoding)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train lstm_net_50\n",
    "models.train(lstm_net_50, train_articles, char_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train lstm_net_100\n",
    "models.train(lstm_net_100, train_articles, char_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
